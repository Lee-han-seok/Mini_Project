{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 이미지 크롤링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 구글"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google_images_download import google_images_download\n",
    "import ssl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "def imageCrawling(keyword, dir):\n",
    "    response = google_images_download.googleimagesdownload()\n",
    "    \n",
    "    arguments = {\"keywords\":keyword,\n",
    "                \"limit\":100,\n",
    "                \"print_urls\":True,\n",
    "                \"no_directory\":True,\n",
    "                \"output_directory\":dir,\n",
    "                \"chromedriver\":\"/Users/Administrator/chromedriver.exe\"}\n",
    "    paths = response.download(arguments)\n",
    "    print(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ['앤헤서웨이',\n",
    "        '엠마왓슨',\n",
    "        '앤마리',\n",
    "        '나탈리포트만',\n",
    "        '크리스틴벨',\n",
    "        '크리스틴스튜어트',\n",
    "        '레이첼 맥아담스',\n",
    "        '바바라 팔빈',\n",
    "        '스칼렛 요한슨',\n",
    "        '미란다커',\n",
    "        '제시카 알바',\n",
    "        '태일러 스위프트',\n",
    "        '제니퍼 로렌스',\n",
    "        '마리옹 꼬띠아르',\n",
    "        '카야 스코델라리오',\n",
    "        '니콜 키드먼',\n",
    "        '엠버 허드',\n",
    "        '에바 그린'\n",
    "        '빌리 아일리시',\n",
    "        '메간폭스',\n",
    "        '칼리레이젭슨',\n",
    "        '케이티페리']\n",
    "\n",
    "for i in range(len(data)):\n",
    "    imageCrawling(data[i], '/Users/Administrator/딥러닝과 인공지능/noise data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "search_item = '안젤리나졸리'\n",
    "url = 'https://www.google.com/search?q=%EC%95%88%EC%A0%A4%EB%A6%AC%EB%82%98%EC%A1%B8%EB%A6%AC&sxsrf=ACYBGNRAQssCZN6IIoG3dnuZTETJwDSeUg:1576039065200&source=lnms&tbm=isch&sa=X&ved=2ahUKEwjsn4iF46zmAhVUPHAKHQ-GDy0Q_AUoAXoECBAQAw&biw=640&bih=608'\n",
    "browser = webdriver.Chrome('C:/Users/Administrator/chromedriver.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "elem = browser.find_element_by_tag_name('body')\n",
    "\n",
    "no_of_pagedowns = 1000\n",
    "while no_of_pagedowns:\n",
    "    elem.send_keys(Keys.PAGE_DOWN)\n",
    "    time.sleep(0.5)\n",
    "    no_of_pagedowns-=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "os.chdir('C:/Users/Administrator/딥러닝과 인공지능/Angelina Jolie Crawling')\n",
    "\n",
    "url_list = ['https://search.daum.net/search?w=img&nil_search=btn&DA=STC&enc=utf8&q=%EC%95%88%EC%A0%A4%EB%A6%AC%EB%82%98%EC%A1%B8%EB%A6%AC&FaceType=y&sd=20181211000000&ed=20191211235959&period=u',\n",
    "           'https://search.daum.net/search?w=img&nil_search=btn&DA=STC&enc=utf8&q=%EC%95%88%EC%A0%A4%EB%A6%AC%EB%82%98%EC%A1%B8%EB%A6%AC&FaceType=y&sd=20171211000000&ed=20181211235959&period=u',\n",
    "           'https://search.daum.net/search?w=img&nil_search=btn&DA=STC&enc=utf8&q=%EC%95%88%EC%A0%A4%EB%A6%AC%EB%82%98%EC%A1%B8%EB%A6%AC&FaceType=y&sd=20161211000000&ed=20171211235959&period=u',\n",
    "           'https://search.daum.net/search?w=img&nil_search=btn&DA=STC&enc=utf8&q=%EC%95%88%EC%A0%A4%EB%A6%AC%EB%82%98%EC%A1%B8%EB%A6%AC&FaceType=y&sd=20151211000000&ed=20161211235959&period=u',\n",
    "           'https://search.daum.net/search?w=img&nil_search=btn&DA=STC&enc=utf8&q=%EC%95%88%EC%A0%A4%EB%A6%AC%EB%82%98%EC%A1%B8%EB%A6%AC&FaceType=y&sd=20141211000000&ed=20151211235959&period=u',\n",
    "           'https://search.daum.net/search?w=img&nil_search=btn&DA=STC&enc=utf8&q=%EC%95%88%EC%A0%A4%EB%A6%AC%EB%82%98%EC%A1%B8%EB%A6%AC&FaceType=y&sd=20131211000000&ed=20141211235959&period=u',\n",
    "           'https://search.daum.net/search?w=img&nil_search=btn&DA=STC&enc=utf8&q=%EC%95%88%EC%A0%A4%EB%A6%AC%EB%82%98%EC%A1%B8%EB%A6%AC&FaceType=y&sd=20121211000000&ed=20131211235959&period=u',\n",
    "           'https://search.daum.net/search?w=img&nil_search=btn&DA=STC&enc=utf8&q=%EC%95%88%EC%A0%A4%EB%A6%AC%EB%82%98%EC%A1%B8%EB%A6%AC&FaceType=y&sd=20111211000000&ed=20121211235959&period=u',\n",
    "           'https://search.daum.net/search?w=img&nil_search=btn&DA=STC&enc=utf8&q=%EC%95%88%EC%A0%A4%EB%A6%AC%EB%82%98%EC%A1%B8%EB%A6%AC&FaceType=y&sd=20101211000000&ed=20111211235959&period=u']\n",
    "count = 0\n",
    "for url in url_list:\n",
    "    browser.get(url)\n",
    "    ## 스크롤\n",
    "    while True:\n",
    "        try:\n",
    "            btn_click = browser.find_element_by_css_selector('a.expender.open')\n",
    "            btn_click.send_keys('\\n')\n",
    "            time.sleep(0.5)\n",
    "        except:\n",
    "            break\n",
    "    \n",
    "    ## link\n",
    "    img_link = browser.find_elements_by_class_name('thumb_img')\n",
    "    link = []\n",
    "    for i in range(len(img_link)):\n",
    "        link.append(img_link[i].get_attribute('src'))\n",
    "    \n",
    "    ## 저장\n",
    "    for url in link:\n",
    "        count+=1\n",
    "        urllib.request.urlretrieve(url, 'img'+str(count)+'.jpg')\n",
    "    count = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "os.chdir('C:/Users/Administrator/딥러닝과 인공지능/Angelina Jolie Crawling')\n",
    "\n",
    "url_list = ['https://search.daum.net/search?w=img&nil_search=btn&DA=STC&enc=utf8&q=%EC%95%88%EC%A0%A4%EB%A6%AC%EB%82%98%EC%A1%B8%EB%A6%AC&FaceType=y&sd=20091211000000&ed=20101211235959&period=u',\n",
    "           'https://search.daum.net/search?w=img&nil_search=btn&DA=STC&enc=utf8&q=%EC%95%88%EC%A0%A4%EB%A6%AC%EB%82%98%EC%A1%B8%EB%A6%AC&FaceType=y&sd=20081211000000&ed=20091211235959&period=u',\n",
    "           'https://search.daum.net/search?w=img&nil_search=btn&DA=STC&enc=utf8&q=%EC%95%88%EC%A0%A4%EB%A6%AC%EB%82%98%EC%A1%B8%EB%A6%AC&FaceType=y&sd=20071211000000&ed=20081211235959&period=u',\n",
    "           'https://search.daum.net/search?w=img&nil_search=btn&DA=STC&enc=utf8&q=%EC%95%88%EC%A0%A4%EB%A6%AC%EB%82%98%EC%A1%B8%EB%A6%AC&FaceType=y&sd=20061211000000&ed=20071211235959&period=u',\n",
    "]\n",
    "for url in url_list:\n",
    "    browser.get(url)\n",
    "    ## 스크롤\n",
    "    while True:\n",
    "        try:\n",
    "            btn_click = browser.find_element_by_css_selector('a.expender.open')\n",
    "            btn_click.send_keys('\\n')\n",
    "            time.sleep(0.5)\n",
    "        except:\n",
    "            break\n",
    "    \n",
    "    ## link\n",
    "    img_link = browser.find_elements_by_class_name('thumb_img')\n",
    "    link = []\n",
    "    for i in range(len(img_link)):\n",
    "        link.append(img_link[i].get_attribute('src'))\n",
    "    \n",
    "    ## 저장\n",
    "    for url in link:\n",
    "        count+=1\n",
    "        urllib.request.urlretrieve(url, 'img'+str(count)+'.jpg')\n",
    "    count = count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 얼굴인식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "colab에서 학습후 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keras.preprocessing import image\n",
    "\n",
    "root_dir = '/Users/Administrator/딥러닝과 인공지능/Angelina Jolie Crawling'\n",
    "\n",
    "fnames = sorted([os.path.join(root_dir, fname) for fname in os.listdir(root_dir)])\n",
    "\n",
    "img_path = fnames[1]\n",
    "img = image.load_img(img_path, target_size=(128,128))\n",
    "\n",
    "img_list = []\n",
    "for fname in fnames:\n",
    "    img = image.load_img(fname, target_size=(128,128))\n",
    "    x = image.img_to_array(img)\n",
    "    img_list.append(x)\n",
    "\n",
    "x = np.asarray(img_list)\n",
    "x = x.astype('float32')/255.\n",
    "\n",
    "# plot\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "imgplot = plt.imshow(image.array_to_img(x[0]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "saved_model = load_model('model-24.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = saved_model.predict(x)\n",
    "\n",
    "import numpy as np\n",
    "pred1 = np.where(prob > .5, 1, 0)\n",
    "pred1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(pred1, columns=['label'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames1 = ['img{}.jpg'.format(i) for i in range(len(df))]\n",
    "fnames1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_fnames1 = pd.DataFrame(fnames1, columns=['name'])\n",
    "df_fnames1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_con = pd.concat([df_fnames1, df], axis=1)\n",
    "df_con.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "predict_dir = '/Users/Administrator/딥러닝과 인공지능/얼굴판별_CNN'\n",
    "\n",
    "for i in range(len(df_con)):\n",
    "    if df_con.iloc[i,1] == 1:\n",
    "        fname = df_con.iloc[i,0]\n",
    "        src = os.path.join(root_dir, fname)\n",
    "        dst = os.path.join(predict_dir, fname)\n",
    "        shutil.copyfile(src, dst)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import model_from_json \n",
    "json_file = open(\"VGGmodel.json\", \"r\") \n",
    "loaded_model_json = json_file.read() \n",
    "json_file.close() \n",
    "loaded_model = model_from_json(loaded_model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.load_weights(\"VGGmodel.h5\")\n",
    "loaded_model.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = loaded_model.predict(x)\n",
    "\n",
    "import numpy as np\n",
    "pred = np.where(prob > .5, 1, 0)\n",
    "pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(pred)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames1 = ['img{}.jpg'.format(i) for i in range(len(df1))]\n",
    "fnames1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_fnames1 = pd.DataFrame(fnames1, columns=['name'])\n",
    "df_fnames1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_con = pd.concat([df_fnames1, df1], axis=1)\n",
    "df_con.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "predict_dir = '/Users/Administrator/딥러닝과 인공지능/얼굴판별_VGG'\n",
    "\n",
    "for i in range(len(df_con)):\n",
    "    if df_con.iloc[i,1] == 1:\n",
    "        fname = df_con.iloc[i,0]\n",
    "        src = os.path.join(root_dir, fname)\n",
    "        dst = os.path.join(predict_dir, fname)\n",
    "        shutil.copyfile(src, dst)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. face crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import cv2 \n",
    "import numpy as np\n",
    "\n",
    "## original file\n",
    "original_dir = 'C:/jolie_project/Angelina Jolie_before crop'\n",
    "\n",
    "## copy file\n",
    "root_dir = 'C:/jolie_project/Angelina Jolie_after crop' \n",
    "\n",
    "fnames = sorted([os.path.join(original_dir, fname) for fname in os.listdir(original_dir)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10355"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 개별 사진 확인: test\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "img = cv2.imread(fnames[6903])\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "time.sleep(1)\n",
    "faces = face_cascade.detectMultiScale(gray, 1.3,5)\n",
    "\n",
    "count = 0\n",
    "if len(faces) > 0: \n",
    "    for (x,y,w,h) in faces:\n",
    "        # cv2.rectangle(img, (x,y), (x+w, y+h), (255,0,0),2)\n",
    "        cropped = img[y:y+h,x:x+w]  \n",
    "        time.sleep(1)\n",
    "        try:\n",
    "            cv2.imwrite(os.path.join(crop_train_dir, 'crop_train' + str(count) + \".jpg\"), cropped)\n",
    "            count +=1\n",
    "        except:\n",
    "            pass\n",
    "else:\n",
    "    pass\n",
    "cv2.imshow('Image View',cropped)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## roop\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades+'haarcascade_frontalface_default.xml') \n",
    "                ## haarcascade~.xml: Andeep에 저장\n",
    "\n",
    "count = 3103\n",
    "for fname in fnames[10006:len(fnames)]:\n",
    "    try: \n",
    "        img = cv2.imread(fname)\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        time.sleep(1)\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.3,5)\n",
    "\n",
    "        if len(faces) > 0: \n",
    "            for (x,y,w,h) in faces:\n",
    "                cropped1 = img[y:y+h,x:x+w]  \n",
    "                time.sleep(1)\n",
    "\n",
    "                ## crop한 사진 copy\n",
    "                try:\n",
    "                    cv2.imwrite(os.path.join(root_dir, 'crop_han' + str(count) + \".jpg\"), cropped1)\n",
    "                    count +=1\n",
    "                    time.sleep(1)\n",
    "                    continue\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to generative adversarial networks\n",
    "\n",
    "This notebook contains the second code sample found in Chapter 8, Section 5 of [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff). Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments.\n",
    "\n",
    "---\n",
    "[...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A schematic GAN implementation\n",
    "\n",
    "\n",
    "In what follows, we explain how to implement a GAN in Keras, in its barest form -- since GANs are quite advanced, diving deeply into the \n",
    "technical details would be out of scope for us. Our specific implementation will be a deep convolutional GAN, or DCGAN: a GAN where the \n",
    "generator and discriminator are deep convnets. In particular, it leverages a `Conv2DTranspose` layer for image upsampling in the generator.\n",
    "\n",
    "We will train our GAN on images from CIFAR10, a dataset of 50,000 32x32 RGB images belong to 10 classes (5,000 images per class). To make \n",
    "things even easier, we will only use images belonging to the class \"frog\".\n",
    "\n",
    "Schematically, our GAN looks like this:\n",
    "\n",
    "* A `generator` network maps vectors of shape `(latent_dim,)` to images of shape `(32, 32, 3)`.\n",
    "* A `discriminator` network maps images of shape (32, 32, 3) to a binary score estimating the probability that the image is real.\n",
    "* A `gan` network chains the generator and the discriminator together: `gan(x) = discriminator(generator(x))`. Thus this `gan` network maps \n",
    "latent space vectors to the discriminator's assessment of the realism of these latent vectors as decoded by the generator.\n",
    "* We train the discriminator using examples of real and fake images along with \"real\"/\"fake\" labels, as we would train any regular image \n",
    "classification model.\n",
    "* To train the generator, we use the gradients of the generator's weights with regard to the loss of the `gan` model. This means that, at \n",
    "every step, we move the weights of the generator in a direction that will make the discriminator more likely to classify as \"real\" the \n",
    "images decoded by the generator. I.e. we train the generator to fool the discriminator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A bag of tricks\n",
    "\n",
    "\n",
    "Training GANs and tuning GAN implementations is notoriously difficult. There are a number of known \"tricks\" that one should keep in mind. \n",
    "Like most things in deep learning, it is more alchemy than science: these tricks are really just heuristics, not theory-backed guidelines. \n",
    "They are backed by some level of intuitive understanding of the phenomenon at hand, and they are known to work well empirically, albeit not \n",
    "necessarily in every context.\n",
    "\n",
    "Here are a few of the tricks that we leverage in our own implementation of a GAN generator and discriminator below. It is not an exhaustive \n",
    "list of GAN-related tricks; you will find many more across the GAN literature.\n",
    "\n",
    "* We use `tanh` as the last activation in the generator, instead of `sigmoid`, which would be more commonly found in other types of models.\n",
    "* We sample points from the latent space using a _normal distribution_ (Gaussian distribution), not a uniform distribution.\n",
    "* Stochasticity is good to induce robustness. Since GAN training results in a dynamic equilibrium, GANs are likely to get \"stuck\" in all sorts of ways. \n",
    "Introducing randomness during training helps prevent this. We introduce randomness in two ways: 1) we use dropout in the discriminator, 2) \n",
    "we add some random noise to the labels for the discriminator.\n",
    "* Sparse gradients can hinder GAN training. In deep learning, sparsity is often a desirable property, but not in GANs. There are two things \n",
    "that can induce gradient sparsity: 1) max pooling operations, 2) ReLU activations. Instead of max pooling, we recommend using strided \n",
    "convolutions for downsampling, and we recommend using a `LeakyReLU` layer instead of a ReLU activation. It is similar to ReLU but it \n",
    "relaxes sparsity constraints by allowing small negative activation values.\n",
    "* In generated images, it is common to see \"checkerboard artifacts\" caused by unequal coverage of the pixel space in the generator. To fix \n",
    "this, we use a kernel size that is divisible by the stride size, whenever we use a strided `Conv2DTranpose` or `Conv2D` in both the \n",
    "generator and discriminator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The generator\n",
    "\n",
    "\n",
    "First, we develop a `generator` model, which turns a vector (from the latent space -- during training it will sampled at random) into a \n",
    "candidate image. One of the many issues that commonly arise with GANs is that the generator gets stuck with generated images that look like \n",
    "noise. A possible solution is to use dropout on both the discriminator and generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "\n",
    "latent_dim = 100\n",
    "height = 128\n",
    "width = 128\n",
    "channels = 3\n",
    "\n",
    "generator_input = keras.Input(shape=(latent_dim,))\n",
    "\n",
    "# First, transform the input into a 16x16 128-channels feature map\n",
    "x = layers.Dense(128 * 64 * 64)(generator_input)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Reshape((64, 64, 128))(x)\n",
    "\n",
    "# Then, add a convolution layer\n",
    "x = layers.Conv2D(256, 5, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "# Upsample to 32x32\n",
    "x = layers.Conv2DTranspose(256, 2, strides=2, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "# Few more conv layers\n",
    "x = layers.Conv2D(256, 5, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(256, 5, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "# Produce a 32x32 1-channel feature map\n",
    "x = layers.Conv2D(channels, 7, activation='tanh', padding='same')(x)\n",
    "generator = keras.models.Model(generator_input, x)\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The discriminator\n",
    "\n",
    "\n",
    "Then, we develop a `discriminator` model, that takes as input a candidate image (real or synthetic) and classifies it into one of two \n",
    "classes, either \"generated image\" or \"real image that comes from the training set\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_input = layers.Input(shape=(height, width, channels))\n",
    "x = layers.Conv2D(128, 3)(discriminator_input)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(128, 4, strides=2)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(128, 4, strides=2)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(128, 4, strides=2)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Flatten()(x)\n",
    "\n",
    "# One dropout layer - important trick!\n",
    "x = layers.Dropout(0.4)(x)\n",
    "\n",
    "# Classification layer\n",
    "x = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "discriminator = keras.models.Model(discriminator_input, x)\n",
    "discriminator.summary()\n",
    "\n",
    "# To stabilize training, we use learning rate decay\n",
    "# and gradient clipping (by value) in the optimizer.\n",
    "discriminator_optimizer = keras.optimizers.RMSprop(lr=0.0008, clipvalue=1.0, decay=1e-8)\n",
    "discriminator.compile(optimizer=discriminator_optimizer, loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The adversarial network\n",
    "\n",
    "Finally, we setup the GAN, which chains the generator and the discriminator. This is the model that, when trained, will move the generator \n",
    "in a direction that improves its ability to fool the discriminator. This model turns latent space points into a classification decision, \n",
    "\"fake\" or \"real\", and it is meant to be trained with labels that are always \"these are real images\". So training `gan` will updates the \n",
    "weights of `generator` in a way that makes `discriminator` more likely to predict \"real\" when looking at fake images. Very importantly, we \n",
    "set the discriminator to be frozen during training (non-trainable): its weights will not be updated when training `gan`. If the \n",
    "discriminator weights could be updated during this process, then we would be training the discriminator to always predict \"real\", which is \n",
    "not what we want!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set discriminator weights to non-trainable\n",
    "# (will only apply to the `gan` model)\n",
    "discriminator.trainable = False\n",
    "\n",
    "gan_input = keras.Input(shape=(latent_dim,))\n",
    "gan_output = discriminator(generator(gan_input))\n",
    "gan = keras.models.Model(gan_input, gan_output)\n",
    "\n",
    "gan_optimizer = keras.optimizers.RMSprop(lr=0.0004, clipvalue=1.0, decay=1e-8)\n",
    "gan.compile(optimizer=gan_optimizer, loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator(gan_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#discriminator.trainable=True\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to train your DCGAN\n",
    "\n",
    "Now we can start training. To recapitulate, this is schematically what the training loop looks like:\n",
    "\n",
    "```\n",
    "for each epoch:\n",
    "    * Draw random points in the latent space (random noise).\n",
    "    * Generate images with `generator` using this random noise.\n",
    "    * Mix the generated images with real ones.\n",
    "    * Train `discriminator` using these mixed images, with corresponding targets, either \"real\" (for the real images) or \"fake\" (for the generated images).\n",
    "    * Draw new random points in the latent space.\n",
    "    * Train `gan` using these random vectors, with targets that all say \"these are real images\". This will update the weights of the generator (only, since discriminator is frozen inside `gan`) to move them towards getting the discriminator to predict \"these are real images\" for generated images, i.e. this trains the generator to fool the discriminator.\n",
    "```\n",
    "\n",
    "Let's implement it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print (os.getcwd()) #현재 디렉토리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 split(처음에만 하고 안해도됨)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "# # Creating Train / Val / Test folders (One time use)\n",
    "root_dir = 'C:/jolie_project/Angelina Jolie Copy'\n",
    "\n",
    "os.makedirs(root_dir +'/train')\n",
    "os.makedirs(root_dir +'/test')\n",
    "\n",
    "# Creating partitions of the data after shuffeling\n",
    "src = 'C:/jolie_project/Angelina Jolie_after crop' # Folder to copy images from\n",
    "\n",
    "allFileNames = os.listdir(src)\n",
    "np.random.shuffle(allFileNames)\n",
    "train_FileNames, test_FileNames = np.split(np.array(allFileNames),\n",
    "                                           [int(len(allFileNames)*0.8)])\n",
    "\n",
    "train_FileNames = [src+'/'+ name for name in train_FileNames.tolist()]\n",
    "test_FileNames = [src+'/' + name for name in test_FileNames.tolist()]\n",
    "\n",
    "print('Total images: ', len(allFileNames))\n",
    "print('Training: ', len(train_FileNames))\n",
    "print('Testing: ', len(test_FileNames))\n",
    "\n",
    "# Copy-pasting images\n",
    "for name in train_FileNames:\n",
    "    shutil.copy(name, \"C:/jolie_project/Angelina Jolie Copy/train\")\n",
    "\n",
    "for name in test_FileNames:\n",
    "    shutil.copy(name, \"C:/jolie_project/Angelina Jolie Copy/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 증식(안해도됨)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "root_dir = 'C:/jolie_project/Angelina Jolie Copy'\n",
    "\n",
    "train_dir = os.path.join(root_dir, 'train')\n",
    "test_dir = os.path.join(root_dir, 'test')\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "# test\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir, \n",
    "    target_size=(128,128), \n",
    "    batch_size=20,\n",
    "    class_mode=None)\n",
    "\n",
    "# train\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255, \n",
    "    rotation_range = 40, \n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=[0.8, 2.0],\n",
    "    horizontal_flip=True,\n",
    ")\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir, \n",
    "    target_size=(150,150),\n",
    "    batch_size=20,\n",
    "    class_mode=None\n",
    ")\n",
    "\n",
    "from keras.preprocessing import image\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "train_real_dir = os.path.join(train_dir, 'all_classes')\n",
    "fnames = sorted([os.path.join(train_real_dir, fname) for fname in os.listdir(train_real_dir)])\n",
    "\n",
    "img_path = fnames[1]\n",
    "img = image.load_img(img_path, target_size=(150,150))\n",
    "\n",
    "x=image.img_to_array(img)\n",
    "x=x.reshape((1,)+x.shape)\n",
    "i=0\n",
    "for batch in train_datagen.flow(x, batch_size=1):\n",
    "    plt.figure(i)\n",
    "    imgplot = plt.imshow(image.array_to_img(batch[0]))\n",
    "    i += 1\n",
    "    if i % 4 == 0:\n",
    "        break\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "안해도됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir, \n",
    "    target_size=(128,128),\n",
    "    class_mode=None)\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir, \n",
    "    target_size=(150,150), \n",
    "    class_mode=None)\n",
    "\n",
    "imgplot = plt.imshow(image.array_to_img(train_generator[0][7]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미지 -> array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keras.preprocessing import image\n",
    "\n",
    "root_dir = 'C:/jolie_project/Angelina Jolie Copy'\n",
    "\n",
    "train_dir = os.path.join(root_dir, 'train')\n",
    "test_dir = os.path.join(root_dir, 'test')\n",
    "\n",
    "train_real_dir = os.path.join(train_dir, 'all_classes')\n",
    "fnames = sorted([os.path.join(train_real_dir, fname) for fname in os.listdir(train_real_dir)])\n",
    "\n",
    "img_path = fnames[1]\n",
    "img = image.load_img(img_path, target_size=(128,128))\n",
    "\n",
    "img_list = []\n",
    "for fname in fnames:\n",
    "    img = image.load_img(fname, target_size=(128,128))\n",
    "    x = image.img_to_array(img)\n",
    "    img_list.append(x)\n",
    "\n",
    "x_train = np.asarray(img_list)\n",
    "x_train = img_arr.astype('float32')/255.\n",
    "\n",
    "# plot\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "imgplot = plt.imshow(image.array_to_img(img_arr[0]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(x_train[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 5000\n",
    "batch_size = 20\n",
    "save_dir = './gan_images/'\n",
    "\n",
    "# Start training loop\n",
    "start = 0\n",
    "for step in range(iterations):\n",
    "    # Sample random points in the latent space\n",
    "    random_latent_vectors = np.random.normal(size=(batch_size, latent_dim))\n",
    "\n",
    "    # Decode them to fake images\n",
    "    generated_images = generator.predict(random_latent_vectors)\n",
    "    generated_images = (generated_images+1)/2.\n",
    "    # Combine them with real images\n",
    "    stop = start + batch_size\n",
    "    real_images = x_train[start: stop]\n",
    "    combined_images = np.concatenate([generated_images, real_images])\n",
    "\n",
    "    # Assemble labels discriminating real from fake images\n",
    "    labels = np.concatenate([np.ones((batch_size, 1)),\n",
    "                             np.zeros((batch_size, 1))])\n",
    "    # Add random noise to the labels - important trick!\n",
    "    labels += 0.05 * np.random.random(labels.shape)\n",
    "\n",
    "    # Train the discriminator\n",
    "    d_loss = discriminator.train_on_batch(combined_images, labels)\n",
    "\n",
    "    # sample random points in the latent space\n",
    "    random_latent_vectors = np.random.normal(size=(batch_size, latent_dim))\n",
    "\n",
    "    # Assemble labels that say \"all real images\"\n",
    "    misleading_targets = np.zeros((batch_size, 1))\n",
    "\n",
    "    # Train the generator (via the gan model,\n",
    "    # where the discriminator weights are frozen)\n",
    "    a_loss = gan.train_on_batch(random_latent_vectors, misleading_targets)\n",
    "    \n",
    "    start += batch_size\n",
    "    if start > len(x_train) - batch_size:\n",
    "        start = 0\n",
    "\n",
    "    # Occasionally save / plot\n",
    "    if step % 100 == 0:\n",
    "        # Save model weights\n",
    "        gan.save_weights('gan.h5')\n",
    "\n",
    "        # Print metrics\n",
    "        print('discriminator loss at step %s: %s' % (step, d_loss))\n",
    "        print('adversarial loss at step %s: %s' % (step, a_loss))\n",
    "\n",
    "        # Save one generated image\n",
    "        img = image.array_to_img(generated_images[0] * 255., scale=False)\n",
    "        img.save(os.path.join(save_dir, 'generated_jolie' + str(step) + '.png'))\n",
    "\n",
    "        # Save one real image, for comparison\n",
    "        img = image.array_to_img(real_images[0] * 255., scale=False)\n",
    "        img.save(os.path.join(save_dir, 'real_jolie' + str(step) + '.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "imgplot = plt.imshow(image.array_to_img(generated_images[17] * 255., scale=False))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's display a few of our fake images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample random points in the latent space\n",
    "random_latent_vectors = np.random.normal(size=(10, latent_dim))\n",
    "\n",
    "# Decode them to fake images\n",
    "generated_images = generator.predict(random_latent_vectors)\n",
    "\n",
    "for i in range(generated_images.shape[0]):\n",
    "    img = image.array_to_img(generated_images[i] * 255., scale=False)\n",
    "#    img = image.array_to_img(generated_images[i], scale=True)\n",
    "    plt.figure()\n",
    "    plt.imshow(img)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Froggy with some pixellated artifacts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 얼굴인식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "tf2.0-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
